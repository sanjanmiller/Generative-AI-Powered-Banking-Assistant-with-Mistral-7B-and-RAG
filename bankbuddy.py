# -*- coding: utf-8 -*-
"""BankBuddy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ANA0nlwE0uAsLa2fFtlIHA-vwRh6Zvyt
"""

!pip install transformers accelerate sentence-transformers chromadb gradio

persist_dir = "/content/drive/MyDrive/chroma_db"

#Load Persisted ChromaDB Collection

import chromadb

client = chromadb.PersistentClient(path=persist_dir)
collection = client.get_collection("banking_faqs")
print("Collection loaded. Number of chunks:", len(collection.get()['documents']))

from huggingface_hub import login
login("enter your hugging face token here")

MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=torch.float16,
    use_auth_token=True
)
print("Mistral 7B Instruct v0.1 loaded successfully!")

#Banking keywords
banking_keywords = [
    "account", "savings", "checking", "CD", "certificate of deposit", "loan", "mortgage",
    "overdraft", "interest rate", "ATM", "debit card", "credit card", "FDIC", "ETAs",
    "IRAs", "Treasury bills", "bank fees", "online banking", "ChexSystems", "financial institution",
    "deposit", "withdrawal", "minimum balance", "money market", "interest", "penalty",
    "transfer", "funds", "financing", "bank account", "account verification"
]
print(f"Total banking keywords in the chatbot: {len(banking_keywords)}")

#Filter function
def is_banking_query(user_query):
    user_query_lower = user_query.lower()
    return any(keyword.lower() in user_query_lower for keyword in banking_keywords)

print(is_banking_query("What is the minimum balance for savings account?"))
print(is_banking_query("Who won the Champions League last year?"))

def retrieve_context(query, k=3):
    # Embed the query
    query_embedding = embed_text([query])[0]

    # Retrieve top-k relevant chunks
    results = collection.query(query_embeddings=[query_embedding], n_results=k)
    unique_chunks = list(dict.fromkeys(results['documents'][0]))
    return "\n".join(unique_chunks)

from sentence_transformers import SentenceTransformer

# Load your embedding mode
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def embed_text(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).tolist()

# Retrieve top-k relevant chunks from ChromaDB
def retrieve_context(query, k=3):
    # Embed the query
    query_embedding = embed_text([query])[0]

    # Query ChromaDB collection
    results = collection.query(query_embeddings=[query_embedding], n_results=k)

    # Remove duplicates and join the text
    unique_chunks = list(dict.fromkeys(results['documents'][0]))
    return "\n".join(unique_chunks)

sample_query = "What is the minimum balance for a savings account?"
print(retrieve_context(sample_query))

# RAG + Mistral response generator
def generate_response(user_query):
    # Check if query is banking-related
    if not is_banking_query(user_query):
        return "I'm designed to answer **banking-related questions only**. Please ask about accounts, loans, interest rates, or other banking topics."

    # Retrieve context from ChromaDB
    context = retrieve_context(user_query, k=3)

    # Build prompt (customer-friendly)
    prompt = f"""
You are an expert banking assistant.

Your goal is to provide **clear, accurate, and beginner-friendly answers** based on the context provided.
Ensure the answer is **complete**, **well-structured**, and does **not end abruptly**.
Avoid giving short or stripped answers. Your response should read naturally, like a helpful expert explaining to a curious customer.

Use simple language, short sentences, and examples where necessary.
Avoid complex jargon unless it is essential â€” and if you must use technical terms, explain them briefly.

---
Context:
{context}

Question: {user_query}
Answer:
"""

    # Generate answer using Mistral 7B
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
    output = model.generate(
        input_ids,
        max_new_tokens=512,
        temperature=0.5,
        top_p=0.7,
        do_sample=True,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id
    )

    # Decode output and clean
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    final_answer = response.split("Answer:")[-1].strip().split("Question:")[0].strip()

    return final_answer

query1 = "What is the minimum balance for a savings account?"
print("User:", query1)
print("Bot:", generate_response(query1))

query2 = "Who won the Champions League last year?"
print("User:", query2)
print("Bot:", generate_response(query2))

import gradio as gr

# Function for the chatbot
def chatbot_interface(user_input):
    response = generate_response(user_input)
    return user_input, response

with gr.Blocks() as demo:
    gr.Markdown("## BankBuddy ðŸ’° â€” Your Personal Banking Assistant")
    question_box = gr.Textbox(label="Your Question", placeholder="Type your banking question here...")
    answer_box = gr.Textbox(label="Answer", interactive=False)

    submit_btn = gr.Button("Ask")

    submit_btn.click(fn=chatbot_interface, inputs=question_box, outputs=[question_box, answer_box])
    question_box.submit(fn=chatbot_interface, inputs=question_box, outputs=[question_box, answer_box])

demo.launch()